@article{Pearson1901a,
author = {Pearson, Karl},
doi = {10.1080/14786440109462720},
issn = {1941-5982},
journal = {Philosophical Magazine Series 6},
abstract = {this is an "abstract", |pipe|},
language = {en},
month = nov,
number = {11},
pages = {559--572},
publisher = {Taylor \& Francis Group},
title = {{On lines and planes of closest fit to systems of points in space}},
url = {http://www.tandfonline.com/doi/abs/10.1080/14786440109462720\#.VMlOwl7F-oU},
volume = {2},
year = {1901}
}


@BOOK{Goodfellow2016-ac,
  title     = "Deep learning",
  author    = "Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and
               Bengio, Yoshua",
  editor    = "Bach, Francis",
  publisher = "MIT press Cambridge",
  volume    =  1,
  year      =  2016,
  language  = "en"
}


@article{Olson2017,
abstract = {The selection, development, or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from different sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists. The present study introduces an accessible, curated, and developing public benchmark resource to facilitate identification of the strengths and weaknesses of different machine learning methodologies. We compare meta-features among the current set of benchmark datasets in this resource to characterize the diversity of available data. Finally, we apply a number of established machine learning methods to the entire benchmark suite and analyze how datasets and algorithms cluster in terms of performance. From this study, we find that existing benchmarks lack the diversity to properly benchmark machine learning algorithms, and there are several gaps in benchmarking problems that still need to be considered. This work represents another important step towards understanding the limitations of popular benchmarking suites and developing a resource that connects existing benchmarking standards to more diverse and efficient standards in the future.},
author = {Olson, Randal S. and {La Cava}, William and Orzechowski, Patryk and Urbanowicz, Ryan J. and Moore, Jason H.},
doi = {10.1186/s13040-017-0154-4},
issn = {1756-0381},
journal = {BioData Mining},
keywords = {Algorithms,Bioinformatics,Computational Biology/Bioinformatics,Computer Appl. in Life Sciences,Data Mining and Knowledge Discovery},
month = {dec},
number = {1},
pages = {36},
publisher = {BioMed Central},
title = {{PMLB: a large benchmark suite for machine learning evaluation and comparison}},
url = {https://biodatamining.biomedcentral.com/articles/10.1186/s13040-017-0154-4},
volume = {10},
year = {2017}
}



@ARTICLE{Fernandez-Delgado2014-qu,
  title   = "Do we need hundreds of classifiers to solve real world
             classification problems",
  author  = "Fern{\'a}ndez-Delgado, Manuel and Cernadas, Eva and Barro,
             Sen{\'e}n and Amorim, Dinani",
  journal = "J. Mach. Learn. Res.",
  volume  =  15,
  number  =  1,
  pages   = "3133--3181",
  year    =  2014
}


@INCOLLECTION{Tomita2017-mv,
  title     = "{ROFLMAO}: Robust Oblique Forests with Linear {MAtrix}
               Operations",
  booktitle = "Proceedings of the 2017 {SIAM} International Conference on Data
               Mining",
  author    = "Tomita, T and Maggioni, M and Vogelstein, J",
  abstract  = "Abstract Random Forest (RF) remains one of the most widely used
               general purpose classification methods. Two recent large-scale
               empirical studies demonstrated it to be the best overall
               classification method among a variety of methods evaluated. One
               of its main limitations, however, is that it is restricted to
               only axis-aligned recursive partitions of the feature space.
               Consequently, RF is particularly sensitive to the orientation of
               the data. Several studies have proposed ?oblique? decision
               forest methods to address this limitation. However, these
               methods either have a time and space complexity significantly
               greater than RF, are sensitive to unit and scale, or empirically
               do not perform as well as RF on real data. One promising oblique
               method that was proposed alongside the canonical RF method,
               called Forest-RC (F-RC), has not received as much attention by
               the community. Despite it being just as old as RF, virtually no
               studies exist investigating its theoretical or empirical
               performance. In this work, we demonstrate that F-RC empirically
               outperforms RF and another recently proposed oblique method
               called Random Rotation Random Forest, while approximately
               maintaining the same computational complexity. Furthermore, a
               variant of F-RC which rank transforms the data prior to learning
               is especially invariant to affine transformations and robust to
               data corruption. Open source code is available.",
  publisher = "Society for Industrial and Applied Mathematics",
  pages     = "498--506",
  series    = "Proceedings",
  month     =  jun,
  year      =  2017
}

@ARTICLE{Tomita2015-xe,
  title         = "Randomer Forests",
  author        = "Tomita, Tyler M and Browne, James and Shen, Cencheng and
                   Priebe, Carey E and Burns, Randal and Maggioni, Mauro and
                   Vogelstein, Joshua T",
  abstract      = "Ensemble methods -- particularly those based on decision
                   trees -- have recently demonstrated superior performance in
                   a variety of machine learning settings. Specifically, Random
                   Forest (RF) was found to outperform >100 other methods in
                   several manuscripts, and gradient boosting trees have been a
                   crucial component of several recent Kaggle competition
                   victories. Building off these successes and recent advances
                   in sparse learning and random matrix theory, we propose a
                   novel ensemble tree method called ``Randomer Forest''
                   (RerF). The key intuition behind RerF is that we can use
                   sparse linear combinations at each decision node rather than
                   just one feature (as in RF) or all of them (as in Rotation
                   Forests). RerF significantly outperforms other methods on a
                   standard benchmark suite containing 105 problems with
                   varying dimension, sample size, and number of classes.
                   Moreover, we provide an implementation that scales as or
                   more efficiently than other available packages. Via a
                   combination of basic principles, theory, and extensive
                   numerical experiments, we demonstrate why, when, and how
                   RerF achieves its performance properties.",
  month         =  jun,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1506.03410"
}


@MISC{Ter_Braak1998-cc,
  title   = "The objective function of partial least squares regression",
  author  = "ter Braak, Cajo J F and de Jong, Sijmen",
  journal = "J. Chemom.",
  volume  =  12,
  number  =  1,
  pages   = "41--54",
  year    =  1998
}

@ARTICLE{Brereton2014-wr,
  title    = "Partial least squares discriminant analysis: taking the magic
              away: {PLS-DA}: taking the magic away",
  author   = "Brereton, Richard G and Lloyd, Gavin R",
  abstract = "Partial least squares discriminant analysis (PLS-DA) has been
              available for nearly 20?years yet is poorly understood by most
              users. By simple examples, it is shown graphically and
              algebraically that for two equal class sizes, PLS-DA using one
              partial least squares (PLS) component provides equivalent
              classification results to Euclidean distance to centroids, and by
              using all nonzero components to linear discriminant analysis.
              Extensions where there are unequal class sizes and more than two
              classes are discussed including common pitfalls and dilemmas.
              Finally, the problems of overfitting and PLS scores plots are
              discussed. It is concluded that for classification purposes,
              PLS-DA has no significant advantages over traditional procedures
              and is an algorithm full of dangers. It should not be viewed as a
              single integrated method but as step in a full classification
              procedure. However, despite these limitations, PLS-DA can provide
              good insight into the causes of discrimination via weights and
              loadings, which gives it a unique role in exploratory data
              analysis, for example in metabolomics via visualisation of
              significant variables such as metabolites or spectroscopic peaks.
              Copyright ? 2014 John Wiley \& Sons, Ltd.",
  journal  = "J. Chemom.",
  volume   =  28,
  number   =  4,
  pages    = "213--225",
  month    =  apr,
  year     =  2014,
  keywords = "Partial Least Squares; Discrimination; Classification; Two Class
              Classifiers"
}



@misc{uci,
author = "Dheeru, Dua and Karra Taniskidou, Efi",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@misc{Dua2017,
author = "Dheeru, Dua and Karra Taniskidou, Efi",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@techreport{geometry_cca,
     title = {The geometry of kernel canonical correlation analysis},
     author = {M.~Kuss and T.~Graepel},
     year = {2003},
     institution = {Max Planck Institute for Biological Cybernetics},
     number = {TR-108}
}

@article{Shin11,
author = {H.~Shin and R.~L.~Eubank},
journal = {Journal of Statistical Computation and Simulation},
title = {Unit canonical correlations and high-dimensional discriminant analysis},
volume = {81},
pages = {167--178},
year = {2011}
}


@MISC{Bridgeford2018-fq,
  title  = "Linear Optimal {Low-Rank} Projection",
  author = "Bridgeford, Eric W and Tang, Minh and Yim, Jason and Vogelstein,
            Joshua T",
  month  =  may,
  year   =  2018
}


@article{maximum1,
author = {J.~Ahn and J.~S.~Marron},
title = {The maximum data piling direction for discrimination},
journal = {Biometrika},
year = {2010},
pages = {254--259},
volume = {97}
}

@book{mardia,
title = {Multivariate Analysis},
author = {K.~V.~Mardia and J.~T.~Kent and J.~M.~Bibby},
year = {1979},
publisher = {Academic Press}
}


@ARTICLE{Nokleby2015,
  title    = "Discrimination on the Grassmann Manifold: Fundamental Limits of
              Subspace Classifiers",
  author   = "Nokleby, M and Rodrigues, M and Calderbank, R",
  abstract = "We derive fundamental limits on the reliable classification of
              linear and affine subspaces from noisy, linear features. Drawing
              an analogy between discrimination among subspaces and
              communication over vector wireless channels, we define two
              Shannon-inspired characterizations of asymptotic classifier
              performance. First, we define the classification capacity, which
              characterizes the necessary and sufficient conditions for
              vanishing misclassification probability as the signal dimension,
              the number of features, and the number of subspaces to be
              discriminated all approach infinity. Second, we define the
              diversity-discrimination tradeoff, which, by analogy with the
              diversity-multiplexing tradeoff of fading vector channels,
              characterizes relationships between the number of discernible
              subspaces and the misclassification probability as the feature
              noise power approaches zero. We derive upper and lower bounds on
              these quantities which are tight in many regimes. Numerical
              results, including a face recognition application, validate the
              results in practice.",
  journal  = "IEEE Trans. Inf. Theory",
  volume   =  61,
  number   =  4,
  pages    = "2133--2147",
  month    =  apr,
  year     =  2015,
  keywords = "face recognition;feature extraction;image classification;image
              denoising;principal component analysis;wireless
              channels;Grassmann manifold;Shannon-inspired
              characterizations;affine subspaces;asymptotic
              classifier;diversity-discrimination
              tradeoff;diversity-multiplexing tradeoff;face recognition;fading
              vector channels;linear features;linear
              subspaces;misclassification probability;noisy features;subspace
              classifiers;vector wireless channels;Capacity planning;Feature
              extraction;Mutual information;Noise;Noise measurement;Upper
              bound;Vectors;Feature extraction;Machine learning;Subspace
              classification;machine learning;subspace classification"
}


@article{cai-pca-1,
  title     = "Optimal estimation and rank detection for sparse spiked
               covariance matrices",
  author    = "Cai, Tony and Ma, Zongming and Wu, Yihong",
  abstract  = "This paper considers a sparse spiked covariance matrix model in
               the high-dimensional setting and studies the minimax estimation
               of the covariance matrix and the principal subspace as well as
               the minimax rank detection. The optimal rate of convergence for
               estimating the spiked covariance matrix under the spectral norm
               is established, which requires significantly different
               techniques from those for estimating other structured covariance
               matrices such as bandable or sparse covariance matrices. We also
               establish the minimax rate under the spectral norm for
               estimating the principal subspace, the primary object of
               interest in principal component analysis. In addition, the
               optimal rate for the rank detection boundary is obtained. This
               result also resolves the gap in a recent paper by Berthet and
               Rigollet (Ann Stat 41(4):1780--1815, 2013) where the special
               case of rank one is considered.",
  journal   = "Probab. Theory Related Fields",
  publisher = "Springer Berlin Heidelberg",
  volume    =  161,
  number    = "3-4",
  pages     = "781--815",
  month     =  "apr",
  year      =  {2015},
  language  = "en"
}


@article{jung-marron,
author = {S.~Jung and J.~S.~Marron},
title = {{PCA} consistency in high-dimension, low sample size context},
journal = {Annals of Statistics},
volume = {37},
pages = {4104--4130},
year = {2009}
}

@article{cai-zhou-1,
author = {T.~T.~Cai and H.~H.~Zhou},
title = {Optimal rate of convergence for sparse covariance matrix estimation},
volume = {40},
journal = {Annals of Statistics},
pages = {2389--2420},
year = {2012}
}

@article{cai-zhou-2,
author = {T.~T.~Cai and C.~H.~Zhang and H.~H.~Zhou},
title = {Optimal rate of convergence for covariance matrix estimation},
volume = {38},
journal = {Annals of Statistics},
pages = {2118--2144},
year = {2010}
}



@article{chernoff_1952,
  author = {H.~Chernoff},
  title = {A measure of asymptotic efficiency for tests of a hypothesis based on the sum of Observations},
  journal = {Annals of Mathematical Statistics},
  pages = {493--507},
  year = {1952},
  volume = {23}
}

@article{leang-johnson,
author = {C.~C.~Leang and D.~H.~Johnson},
title = {On the asymptotics of {M}-hypothesis Bayesian detection},
journal = {IEEE Transactions on Information Theory},
volume = {43},
pages = {280--282},
year = {1997}
}

@Article{Csizar,
author = {I.~Csiz\'{a}r},
title = {Information-type measures of difference of probability distributions and indirect observations},
journal = {Studia Scientiarum Mathematicarum Hungarica},
year = {1967},
volume = {2},
pages = {229--318}
}

@article{Fisher1925a,
author = {Fisher, R. A.},
doi = {10.1017/S0305004100009580},
file = {::},
issn = {0305-0041},
journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
language = {English},
month = oct,
number = {05},
pages = {700--725},
publisher = {Cambridge University Press},
title = {{Theory of Statistical Estimation}},
url = {http://journals.cambridge.org/abstract\_S0305004100009580},
volume = {22},
year = {1925}
}
@article{Eckart1936a,
author = {Eckart, Carl and Young, Gale},
doi = {10.1007/BF02288367},
issn = {0033-3123},
journal = {Psychometrika},
keywords = {Behavioral Science},
month = sep,
number = {3},
pages = {211--218},
publisher = {Springer New York},
title = {{The approximation of one matrix by another of lower rank}},
url = {http://www.springerlink.com/content/9v4274h33h75lq24/},
volume = {1},
year = {1936}
}
@article{Householder1938,
abstract = {Necessary and sufficient conditions are given for a set of numbers to be the mutual distances of a set of real points in Euclidean space, and matrices are found whose ranks determine the dimension of the smallest Euclidean space containing such points. Methods are indicated for determining the configuration of these points, and for approximating to them by points in a space of lower dimensionality.},
author = {Householder, A S and Young, Gale},
doi = {10.1007/BF02287916},
issn = {00333123},
journal = {Psychometrika},
number = {1},
pages = {19--22},
publisher = {Springer},
title = {{Discussion of a set of points in terms of their mutual distances}},
url = {http://www.springerlink.com/index/10.1007/BF02287916},
volume = {3},
year = {1938}
}
@article{Trunk1979a,
abstract = {In pattern recognition problems it has been noted that beyond a certain point the inclusion of additional parameters (that have been estimated) leads to higher probabilities of error. A simple problem has been formulated where the probability of error approaches zero as the dimensionality increases and all the parameters are known; on the other hand, the probability of error approaches one-half as the dimensionality increases and parameters are estimated.},
author = {Trunk, G V},
institution = {Naval Research Laboratory, Washington, DC 20375.},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {3},
pages = {306--307},
pmid = {21868861},
publisher = {New York, U.S.A.},
title = {{A problem of dimensionality: a simple example.}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4766926},
volume = {1},
year = {1979}
}
@book{Huber1981a,
abstract = {Book Description Other volumes in the Wiley Series in Probability and Mathematical Statistics Abstract Inference UIf Grenander The traditional setting of statistical inference is when both sample space and parameter space are finite dimensional Euclidean spaces or subjects of such spaces. During the last decades, however, a theory has been developed that allows the sample space to be some abstract space. More recently, mathematical techniquesespecially the method of sieveshave been constructed to enable inferences to be made in abstract parameter spaces. This work began with the authors 1950 monograph on inference in stochastic processes (for general sample space) and with the sieve methodology (for general parameter space) that the author and his co-workers at Brown University developed in the 1970s. Both of these cases are studied in this volume, which is the first comprehensive treatment of the subject. 1980 Order Statistics, 2nd Ed. Herbert A. David Presents up-to-date coverage of the theory and applications of ordered random variables and their functions. Develops the distribution theory of order statistics systematically, and treats short-cut methods, robust estimation, life testing, reliability, and extreme-value theory. Applications include procedures for the treatment of outliers and other data analysis techniques. Provides extensive references to the literature and the tables contained therein. Exercises included. 1980 Theory and Applications of Stochastic Differential Equations Zeev Schuss Presents SDE theory through its applications in the physical sciences, e.g., the description of chemical reactions, solid state diffusion and electrical conductivity, population genetics, filtering problems in communication theory, etc. Introduces the stochastic calculus in a simple way, presupposing only basic analysis and probability theory. Shows the role of first passage times and exit distributions in modeling physical phenomena. Introduces analytical methods in order to obtain information on probabilistic quantities such as moments and distribution of first passage times and transition probabilities, and demonstrates the role of partial differential equations. Methods include singular perturbation techniques, old and new asymptotic methods, and boundary layer theory. 1980 Download Description The first systematic, book-length treatment of the subject. Begins with a general introduction and the formal mathematical background behind qualitative and quantitative robustness. Stresses concepts. Provides selected numerical algorithms for computing robust estimates, as well as convergence proofs. Tables contain quantitative robustness information for a variety of estimates.},
author = {Huber, Peter J.},
booktitle = {Analysis},
doi = {10.1002/9780470434697},
isbn = {9780470434697},
number = {3},
pages = {308},
publisher = {Wiley},
series = {Wiley Series in Probability and Statistics},
title = {{Robust Statistics}},
url = {http://doi.wiley.com/10.1002/9780470434697},
volume = {82},
year = {1981}
}
@article{Kohonen1982a,
author = {Kohonen, Teuvo},
doi = {10.1007/BF00337288},
issn = {0340-1200},
journal = {Biological Cybernetics},
number = {1},
pages = {59--69},
title = {{Self-organized formation of topologically correct feature maps}},
url = {http://link.springer.com/10.1007/BF00337288},
volume = {43},
year = {1982}
}
@article{Huber1985a,
author = {Huber, Peter J.},
issn = {2168-8966},
journal = {The Annals of Statistics},
language = {EN},
month = jun,
number = {2},
pages = {435--475},
publisher = {Institute of Mathematical Statistics},
title = {{Projection Pursuit}},
url = {http://projecteuclid.org/euclid.aos/1176349519},
volume = {13},
year = {1985}
}
@article{Friedman1989a,
abstract = {Abstract Linear and quadratic discriminant analysis are considered in the small-sample, high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample-based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved.
Abstract Linear and quadratic discriminant analysis are considered in the small-sample, high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample-based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved.},
author = {Friedman, Jerome H.},
doi = {10.1080/01621459.1989.10478752},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = mar,
number = {405},
pages = {165--175},
publisher = {Taylor \& Francis},
title = {{Regularized Discriminant Analysis}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478752},
volume = {84},
year = {1989}
}
@article{Li1991a,
abstract = {Abstract Modern advances in computing power have greatly widened scientists' scope in gathering and investigating information from many variables, information which might have been ignored in the past. Yet to effectively scan a large pool of variables is not an easy task, although our ability to interact with data has been much enhanced by recent innovations in dynamic graphics. In this article, we propose a novel data-analytic tool, sliced inverse regression (SIR), for reducing the dimension of the input variable x without going through any parametric or nonparametric model-fitting process. This method explores the simplicity of the inverse view of regression; that is, instead of regressing the univariate output variable y against the multivariate x, we regress x against y. Forward regression and inverse regression are connected by a theorem that motivates this method. The theoretical properties of SIR are investigated under a model of the form, y = f(? 1 x, ?, ? K x, $\epsilon$), where the ? k 's are the unknown row vectors. This model looks like a nonlinear regression, except for the crucial difference that the functional form of f is completely unknown. For effectively reducing the dimension, we need only to estimate the space [effective dimension reduction (e.d.r.) space] generated by the ? k 's. This makes our goal different from the usual one in regression analysis, the estimation of all the regression coefficients. In fact, the ? k 's themselves are not identifiable without a specific structural form on f. Our main theorem shows that under a suitable condition, if the distribution of x has been standardized to have the zero mean and the identity covariance, the inverse regression curve, E(x | y), will fall into the e.d.r. space. Hence a principal component analysis on the covariance matrix for the estimated inverse regression curve can be conducted to locate its main orientation, yielding our estimates for e.d.r. directions. Furthermore, we use a simple step function to estimate the inverse regression curve. No complicated smoothing is needed. SIR can be easily implemented on personal computers. By simulation, we demonstrate how SIR can effectively reduce the dimension of the input variable from, say, 10 to K = 2 for a data set with 400 observations. The spin-plot of y against the two projected variables obtained by SIR is found to mimic the spin-plot of y against the true directions very well. A chi-squared statistic is proposed to address the issue of whether or not a direction found by SIR is spurious.
Abstract Modern advances in computing power have greatly widened scientists' scope in gathering and investigating information from many variables, information which might have been ignored in the past. Yet to effectively scan a large pool of variables is not an easy task, although our ability to interact with data has been much enhanced by recent innovations in dynamic graphics. In this article, we propose a novel data-analytic tool, sliced inverse regression (SIR), for reducing the dimension of the input variable x without going through any parametric or nonparametric model-fitting process. This method explores the simplicity of the inverse view of regression; that is, instead of regressing the univariate output variable y against the multivariate x, we regress x against y. Forward regression and inverse regression are connected by a theorem that motivates this method. The theoretical properties of SIR are investigated under a model of the form, y = f(? 1 x, ?, ? K x, $\epsilon$), where the ? k 's are the unknown row vectors. This model looks like a nonlinear regression, except for the crucial difference that the functional form of f is completely unknown. For effectively reducing the dimension, we need only to estimate the space [effective dimension reduction (e.d.r.) space] generated by the ? k 's. This makes our goal different from the usual one in regression analysis, the estimation of all the regression coefficients. In fact, the ? k 's themselves are not identifiable without a specific structural form on f. Our main theorem shows that under a suitable condition, if the distribution of x has been standardized to have the zero mean and the identity covariance, the inverse regression curve, E(x | y), will fall into the e.d.r. space. Hence a principal component analysis on the covariance matrix for the estimated inverse regression curve can be conducted to locate its main orientation, yielding our estimates for e.d.r. directions. Furthermore, we use a simple step function to estimate the inverse regression curve. No complicated smoothing is needed. SIR can be easily implemented on personal computers. By simulation, we demonstrate how SIR can effectively reduce the dimension of the input variable from, say, 10 to K = 2 for a data set with 400 observations. The spin-plot of y against the two projected variables obtained by SIR is found to mimic the spin-plot of y against the true directions very well. A chi-squared statistic is proposed to address the issue of whether or not a direction found by SIR is spurious.},
author = {Li, Ker-Chau},
doi = {10.1080/01621459.1991.10475035},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = jun,
number = {414},
pages = {316--327},
publisher = {Taylor \& Francis},
title = {{Sliced Inverse Regression for Dimension Reduction}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1991.10475035},
volume = {86},
year = {1991}
}
@article{Tibshirani1996,
author = {Tibshirani, Robert},
journal = {Journal of the Royal Statistical Society. Series B},
pages = {267--288},
title = {{Regression Shrinkage and Selection via the Lasso}},
volume = {58},
year = {1996}
}
@article{Belhumeur1997a,
abstract = {We develop a face recognition algorithm which is insensitive to
large variation in lighting direction and facial expression. Taking a
pattern classification approach, we consider each pixel in an image as a
coordinate in a high-dimensional space. We take advantage of the
observation that the images of a particular face, under varying
illumination but fixed pose, lie in a 3D linear subspace of the high
dimensional image space-if the face is a Lambertian surface without
shadowing. However, since faces are not truly Lambertian surfaces and do
indeed produce self-shadowing, images will deviate from this linear
subspace. Rather than explicitly modeling this deviation, we linearly
project the image into a subspace in a manner which discounts those
regions of the face with large deviation. Our projection method is based
on Fisher's linear discriminant and produces well separated classes in a
low-dimensional subspace, even under severe variation in lighting and
facial expressions. The eigenface technique, another method based on
linearly projecting the image space to a low dimensional subspace, has
similar computational requirements. Yet, extensive experimental results
demonstrate that the proposed \&amp;ldquo;Fisherface\&amp;rdquo; method has error
rates that are lower than those of the eigenface technique for tests on
the Harvard and Yale face databases},
author = {Belhumeur, Peter N. and Hespanha, Jo\"{a}o P. and Kriegman, David J.},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Appearance-based vision,Face recognition,Fisher's linear discriminant,Illumination invariance},
number = {7},
pages = {711--720},
pmid = {11148084},
title = {{Eigenfaces vs. fisherfaces: Recognition using class specific linear projection}},
volume = {19},
year = {1997}
}
@article{Olshausen1997a,
abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of astrategy for producing asparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparsecoding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
author = {Olshausen, Bruno A. and Field, David J.},
doi = {10.1016/S0042-6989(97)00169-7},
issn = {00426989},
journal = {Vision Research},
keywords = {coding,gabor-wavelet,natural images,v1},
month = dec,
number = {23},
pages = {3311--3325},
title = {{Sparse coding with an overcomplete basis set: A strategy employed by V1?}},
url = {http://dx.doi.org/10.1016/S0042-6989(97)00169-7},
volume = {37},
year = {1997}
}
@article{Bishop1998a,
abstract = {Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis, which is based on a linear transformation between the latent space and the data space. In this article, we introduce a form of nonlinear latent variable model called the generative topographic mapping, for which the parameters of the model can be determined using the expectation-maximization algorithm. GTM provides a principled alternative to the widely used self-organizing map (SOM) of Kohonen (1982) and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multiphase oil pipeline.},
author = {Bishop, Christopher M. and Svens\'{e}n, Markus and Williams, Christopher K. I.},
doi = {10.1162/089976698300017953},
issn = {0899-7667},
journal = {Neural Computation},
language = {en},
month = jan,
number = {1},
pages = {215--234},
publisher = {MIT Press  238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu},
title = {{GTM: The Generative Topographic Mapping}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017953\#.VMlRX17F-oU},
volume = {10},
year = {1998}
}
@article{Rousseeuw1999a,
abstract = {The minimum covariance determinant (MCD) method of Rousseeuw is a highly robust estimator of multivariate location and scatter. Its objective is to find h observations (out of n) whose covariance matrix has the lowest determinant. Until now, applications of the MCD were hampered by the computation time of existing algorithms, which were limited to a few hundred objects in a few dimensions. We discuss two important applications of larger size, one about a production process at Philips with n = 677 objects and p = 9 variables, and a dataset from astronomy with n = 137,256 objects and p = 27 variables. To deal with such problems we have developed a new algorithm for the MCD, called FAST-MCD. The basic ideas are an inequality involving order statistics and determinants, and techniques which we call “selective iteration” and “nested extensions.” For small datasets, FAST-MCD typically finds the exact MCD, whereas for larger datasets it gives more accurate results than existing algorithms and is faster by orders...},
author = {Rousseeuw, Peter J. and Driessen, Katrien Van},
journal = {Technometrics},
keywords = {Breakdown value,Multivariate location and scatter,Outlier detection,Regression,Robust estimation},
language = {en},
month = mar,
number = {3},
pages = {212--223},
publisher = {Taylor \& Francis Group},
title = {{A Fast Algorithm for the Minimum Covariance Determinant Estimator}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/00401706.1999.10485670\#.VTejtq1Viko},
volume = {41},
year = {1999}
}
@article{Ferrari2010a,
author = {Ferrari, Davide and Yang, Yuhong},
doi = {10.1214/09-AOS687},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {753--783},
title = {{Maximum L q -likelihood estimation}},
url = {http://projecteuclid.org/euclid.aos/1266586613},
volume = {38},
year = {2010}
}
@incollection{abello1998functional,
  title={A functional approach to external graph algorithms},
  author={Abello, James and Buchsbaum, Adam L and Westbrook, Jeffery R},
  booktitle={Algorithms—ESA’98},
  pages={332--343},
  year={1998},
  publisher={Springer}
}


@ARTICLE{Agarwal2014,
  title   = "A Reliable Effective Terascale Linear Learning System",
  author  = "Agarwal, Alekh and Chapelle, Oliveier and Dud{\'\i}k, Miroslav and
             Langford, John",
  journal = "J. Mach. Learn. Res.",
  volume  =  15,
  pages   = "1111--1133",
  year    =  2014
}
@article{abadi2016tensorflow,
  title={Tensorflow: Large-scale machine learning on heterogeneous distributed systems},
  author={Abadi, Mart{\'\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and others},
  journal={arXiv preprint arXiv:1603.04467},
  year={2016}
}

@article{Gretton05,
	Author = {A. Gretton and R. Herbrich and A. Smola and O. Bousquet and B. Scholkopf},
	Date-Added = {2014-07-22 13:44:56 +0000},
	Date-Modified = {2014-07-22 14:06:50 +0000},
	Journal = {Journal of Machine Learning Research},
	Pages = {2075-2129},
	Title = {Kernel methods for measuring independence},
	Volume = {6},
	Year = {2005}}


@ARTICLE{Hastie1996,
  title     = "Discriminant Analysis by Gaussian Mixtures",
  author    = "Hastie, Trevor and Tibshirani, Robert",
  abstract  = "Fisher-Rao linear discriminant analysis (LDA) is a valuable tool
               for multigroup classification. LDA is equivalent to maximum
               likelihood classification assuming Gaussian distributions for
               each class. In this paper, we fit Gaussian mixtures to each
               class to facilitate effective classification in non-normal
               settings, especially when the classes are clustered. Low
               dimensional views are an important by-product of LDA--our new
               techniques inherit this feature. We can control the within-class
               spread of the subclass centres relative to the between-class
               spread. Our technique for fitting these models permits a natural
               blend with nonparametric versions of LDA.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "[Royal Statistical Society, Wiley]",
  volume    =  58,
  number    =  1,
  pages     = "155--176",
  year      =  1996
}



@article{mrcap,
  title={Magnetic Resonance Connectome Automated Pipeline},
  author={Gray, William R and Bogovic, John A and Vogelstein, Joshua T and Landman, Bennett A and Prince, Jerry L and Vogelstein, R Jacob},
  journal={IEEE Pulse},
  volume={3},
  number={2},
  pages={42--48},
  year={2011}
}
@article{migraine,
  archivePrefix = {arXiv},
  arxivId = {1312.4875v1},
  author = {{Gray Roncal}, William and others},
  eprint = {1312.4875v1},
  journal = {Global Conference on Signal and Information Processing},
  title = {{MIGRAINE: MRI Graph Reliability Analysis and Inference for Connectomics}},
  year = {2013}
}


@UNPUBLISHED{Kiar2018-bo,
  title       = "A {High-Throughput} Pipeline Identifies Robust Connectomes But
                 Troublesome Variability",
  author      = "Kiar, Gregory and Bridgeford, Eric and Roncal, Will Gray and
                 {Consortium for Reliability and Reproducibliity (CoRR)} and
                 Chandrashekhar, Vikram and Mhembere, Disa and Ryman, Sephira
                 and Zuo, Xi-Nian and Marguiles, Daniel S and Cameron Craddock,
                 R and Priebe, Carey E and Jung, Rex and Calhoun, Vince and
                 Caffo, Brian and Burns, Randal and Milham, Michael P and
                 Vogelstein, Joshua",
  abstract    = "Modern scientific discovery depends on collecting large
                 heterogeneous datasets with many sources of variability, and
                 applying domain-specific pipelines from which one can draw
                 insight or clinical utility. For example, macroscale
                 connectomics studies require complex pipelines to process raw
                 functional or diffusion data and estimate connectomes.
                 Individual studies tend to customize pipelines to their needs,
                 raising concerns about their reproducibility, and adding to a
                 longer list of factors that may differ across studies
                 (including sampling, experimental design, and data acquisition
                 protocols), resulting in failures to replicate. Mitigating
                 these issues requires multi-study datasets and the development
                 of pipelines that can be applied across them. We developed
                 NeuroData9s MRI to Graphs (NDMG) pipeline using several
                 functional and diffusion studies, including the Consortium for
                 Reliability and Reproducibility, to estimate connectomes.
                 Without any manual intervention or parameter tuning, NDMG ran
                 on 25 different studies (~6,000 scans) from 15 sites, with
                 each scan resulting in a biologically plausible connectome (as
                 assessed by multiple quality assurance metrics at each
                 processing stage). For each study, the connectomes from NDMG
                 are more similar within than across individuals, indicating
                 that NDMG is preserving biological variability. Moreover, the
                 connectomes exhibit near perfect consistency for certain
                 connectional properties across every scan, individual, study,
                 site, and modality; these include stronger ipsilateral than
                 contralateral connections and stronger homotopic than
                 heterotopic connections. Yet, the magnitude of the differences
                 varied across individuals and studies - much more so when
                 pooling data across sites, even after controlling for study,
                 site, and basic demographic variables (i.e., age, sex, and
                 ethnicity). This indicates that other experimental variables
                 (possibly those not measured or reported) are contributing to
                 this variability, which if not accounted for can limit the
                 value of aggregate datasets, as well as expectations regarding
                 the accuracy of findings and likelihood of replication. We,
                 therefore, provide a set of principles to guide the
                 development of pipelines capable of pooling data across
                 studies while maintaining biological variability and
                 minimizing measurement error. This open science approach
                 provides us with an opportunity to understand and eventually
                 mitigate spurious results for both past and future studies.",
  journal     = "bioRxiv",
  pages       = "188706",
  institution = "bioRxiv",
  month       =  apr,
  year        =  2018,
  language    = "en"
}


@article{sic,
	author = {Kiar, Gregory and Gorgolewski, Krzysztof J.
	and Kleissas, Dean and {Gray Roncal}, William
	and Litt, Brian and Wandell, Brian and Poldrack, Russel A.
	and Wiener, Martin and Vogelstein, {R. Jacob}
	and Burns, Randal and Vogelstein, Joshua T.},
	title = {Science In the Cloud (SIC): A use case in MRI Connectomics},
	journal={GigaScience},
	year={2017},
	volume={gix013},
	month={mar},
	doi={10.1093/gigascience/gix013}
}
@article{Hastie2006,
address = {New York, New York, USA},
author = {Hastie, Trevor and Church, Kenneth Ward and Li, Ping and Kdd, Kenneth Church},
doi = {10.1145/1150402.1150436},
isbn = {1595933395},
journal = {Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '06},
keywords = {expectations,matrix b preserves all,much smaller,of,pairwise dis-,provided that r consists,random projections,rates of convergence,sampling,tances of a in,the},
pages = {287},
publisher = {ACM Press},
title = {{Very sparse random projections}},
url = {http://portal.acm.org/citation.cfm?doid=1150402.1150436},
year = {2006}
}
@article{Candes2006b,
author = {Cand{\`{e}}s, Emmanuel J. and Tao, Terence},
doi = {10.1109/TIT.2006.885507},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
keywords = {acknowledgments,c,concentration of measure,convex optimization,dom projections,duality in optimization,e,is partially supported by,linear programming,matrices,national science foundation grants,principle,ran-,random matrices,signal recovery,singular values of random,sparsity,trigonometric expansions,uncertainty},
month = {dec},
number = {12},
pages = {5406--5425},
title = {{Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4016283},
volume = {52},
year = {2006}
}



@ARTICLE{Cannings2015,
  title         = "Random-projection ensemble classification",
  author        = "Cannings, Timothy I and Samworth, Richard J",
  abstract      = "We introduce a very general method for high-dimensional
                   classification, based on careful combination of the results
                   of applying an arbitrary base classifier to random
                   projections of the feature vectors into a lower-dimensional
                   space. In one special case that we study in detail, the
                   random projections are divided into disjoint groups, and
                   within each group we select the projection yielding the
                   smallest estimate of the test error. Our random projection
                   ensemble classifier then aggregates the results of applying
                   the base classifier on the selected projections, with a
                   data-driven voting threshold to determine the final
                   assignment. Our theoretical results elucidate the effect on
                   performance of increasing the number of projections.
                   Moreover, under a boundary condition implied by the
                   sufficient dimension reduction assumption, we show that the
                   test excess risk of the random projection ensemble
                   classifier can be controlled by terms that do not depend on
                   the original data dimension and a term that becomes
                   negligible as the number of projections increases. The
                   classifier is also compared empirically with several other
                   popular high-dimensional classifiers via an extensive
                   simulation study, which reveals its excellent finite-sample
                   performance.",
  month         =  "Apr",
  year          =  {2015},
  journal       = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "1504.04595"
}


@ARTICLE{Craddock2013,
  title    = "Imaging Functional and Structural Connectomes at the Macroscale",
  author   = "Craddock, R Cameron and Jbabdi, Sa{\^a}d Saad and Yan, Chao-Gan
              and Vogelstein, Joshua T and Castellanos, Xavier Francisco and Di
              Martino, Adriana and Kelly, A M Clare and Heberlein, Keith and
              Colcombe, Stanley J and Milham, Michael Peter and Castellanos, F
              Xavier and Di Martino, Adriana and Kelly, Clare and Heberlein,
              Keith and Colcombe, Stanley J and Milham, Michael Peter and
              Castellanos, Xavier Francisco and Di Martino, Adriana and Kelly,
              A M Clare and Heberlein, Keith and Colcombe, Stanley J and
              Milham, Michael Peter",
  abstract = "At macroscopic scales, the human connectome comprises
              anatomically distinct brain areas, the structural pathways
              connecting them and their functional interactions. Annotation of
              phenotypic associations with variation in the connectome and
              cataloging of neurophenotypes promise to transform our
              understanding of the human brain. In this Review, we provide a
              survey of magnetic resonance imaging--based measurements of
              functional and structural connectivity. We highlight emerging
              areas of development and inquiry and emphasize the importance of
              integrating structural and functional perspectives on brain
              architecture.",
  journal  = "Nat. Methods",
  volume   =  10,
  number   =  6,
  pages    = "524--539",
  year     =  2013,
  keywords = "Brain; Brain: cytology; Brain: physiology; Connectome; Humans;
              Magnetic Resonance Imaging; Magnetic Resonance Imaging: methods;
              Phenotype"
}


@ARTICLE{Duarte-Carvajalino2011,
  title     = "Hierarchical topological network analysis of anatomical human
               brain connectivity and differences related to sex and kinship",
  author    = "Duarte-Carvajalino, J M and Jahanshad, Neda",
  journal   = "Neuroimage",
  publisher = "Elsevier Inc.",
  volume    =  59,
  number    =  4,
  pages     = "3784--3804",
  year      =  2011,
  keywords  = "Complex networks; Diffusion weighted MRI; False discovery rate;
               Hierarchical analysis; Sex and kinship brain network
               differences; Topological analysis; anatomical brain connectivity"
}


@ARTICLE{Vogelstein2013,
  title   = "Graph Classification Using {Signal-Subgraphs}: Applications in
             Statistical Connectomics",
  author  = "Vogelstein, J T and Roncal, W G and Vogelstein, R J and Priebe, C
             E",
  journal = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume  =  35,
  number  =  7,
  pages   = "1539--1551",
  year    =  2013
}

@inproceedings{Amdahl1967,
abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
archivePrefix = {arXiv},
arxivId = {arXiv:quant-ph/0611061v2},
author = {Amdahl, Gene M.},
booktitle = {AFIPS Spring Joint Computer Conference, 1967. AFIPS '67 (Spring). Proceedings of the},
doi = {10.1145/1465482.1465560},
eprint = {0611061v2},
isbn = {1558605398},
issn = {18816096},
keywords = {parallel-computing},
pages = {483--485},
pmid = {21914993},
primaryClass = {arXiv:quant-ph},
title = {{Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities}},
url = {http://delivery.acm.org/10.1145/1470000/1465560/p483-amdahl.pdf?ip=202.189.127.238{\&}id=1465560{\&}acc=ACTIVE SERVICE{\&}key=CDD1E79C27AC4E65.DE0A32330AE3471B.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=646862329{\&}CFTOKEN=33418015{\&}{\_}{\_}acm{\_}{\_}=1426882996{\_}a666a43ab4250317fe0},
volume = {30},
year = {1967}
}
@inproceedings{Mika1999a,
abstract = {A non-linear classification technique based on Fisher's discriminant is proposed. The main ingredient is the kernel trick which allows the efficient computation of Fisher discriminant in feature space. The linear classification in feature space corresponds to a (powerful) non-linear decision function in input space. Large scale simulations demonstrate the competitiveness of our approach},
author = {Mika, S. and Ratsch, G. and Weston, J. and Scholkopf, B. and Mullers, K.R.},
booktitle = {Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468)},
doi = {10.1109/NNSP.1999.788121},
isbn = {0-7803-5673-X},
keywords = {Algorithm design and analysis,Bayes methods,Closed-form solution,Computational modeling,Feature extraction,Fisher discriminant analysis,Gaussian distribution,Kernel,Large-scale systems,Principal component analysis,Probability,Support vector machines,decision theory,feature extraction,feature space,input space,kernels,learning (artificial intelligence),linear classification,neural nets,nonlinear classification technique,nonlinear decision function,pattern classification},
pages = {41--48},
publisher = {IEEE},
shorttitle = {Neural Networks for Signal Processing IX, 1999. Pr},
title = {{Fisher discriminant analysis with kernels}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=788121},
year = {1999}
}
@article{Tishby1999a,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0004057v1},
author = {Tishby, Naftali and Pereira, Fernando C and Bialek, William},
eprint = {0004057v1},
journal = {Neural Computation},
pages = {1--16},
primaryClass = {arXiv:physics},
title = {{The information bottleneck method arXiv : physics / 0004057v1 [ physics . data-an ] 24 Apr 2000}},
year = {1999}
}
@book{Anderson1999a,
abstract = {LAPACK is a library of numerical linear algebra subroutines designed for high performance on workstations, vector computers, and shared memory multiprocessors. Release 3.0 of LAPACK introduces new routines and extends the functionality of existing routines. The most significant new routines and functions include: 1. a faster singular value decomposition computed by divide-and-conquer 2. faster routines for solving rank-deficient least squares problems: Using QR with column pivoting using the SVD based on divide-and-conquer 3. new routines for the generalized symmetric eigenproblem: faster routines based on divide-and-conquer routines based on bisection/inverse iteration, for computing part of the spectrum 4. faster routine for the symmetric eigenproblem using "relatively robust eigenvector algorithm" 5. new simple and expert drivers for the generalized nonsymmetric eigenproblem, including error bounds 6. solver for generalized Sylvester equation, used in 5 7.computational routines used in 5 Each Users' Guide comes with a 'Quick Reference Guide' card.},
author = {Anderson, E. and Bai, Z. and Bischof, C. and Blackford, S. and Demmel, J. and Dongarra, J. and Croz, J. Du and Greenbaum, A. and Hammerling, S. and McKenney, A. and Sorensen, D.},
isbn = {0898714478},
pages = {407},
publisher = {SIAM},
title = {{LAPACK Users' Guide: Third Edition}},
url = {https://books.google.com/books?hl=en\&lr=\&id=AZlvEnr9gCgC\&pgis=1},
year = {1999}
}

@BOOK{Lee2007-bw,
  title     = "Nonlinear Dimensionality Reduction (Information Science and
               Statistics)",
  author    = "Lee, John A and Verleysen, Michel",
  publisher = "Springer",
  edition   = "2007 edition",
  month     =  dec,
  year      =  2007,
  language  = "en"
}
@article{Roweis2000a,
abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
annote = {From Duplicate 2 ( },
author = {Roweis, Sam T and Saul, L K},
doi = {10.1126/science.290.5500.2323},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
keywords = {Algorithms,Artificial Intelligence,Face,Humans,Mathematics,Pattern Recognition,Visual},
month = dec,
number = {5500},
pages = {2323--6},
pmid = {11125150},
title = {{Nonlinear dimensionality reduction by locally linear embedding.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11125150},
volume = {290},
year = {2000}
}
@article{Tenenbaum2000a,
abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
annote = {From Duplicate 1 ( },
author = {Tenenbaum, Joshua B and de Silva, V and Langford, John C and Silva, Vin De},
doi = {10.1126/science.290.5500.2319},
isbn = {0036-8075},
issn = {00368075},
journal = {Science},
keywords = {Algorithms,Artificial Intelligence,Face,Humans,Mathematics,Pattern Recognition,Visual,Visual Perception},
month = dec,
number = {5500},
pages = {2319--23},
pmid = {11125149},
title = {{A global geometric framework for nonlinear dimensionality reduction.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11125149},
volume = {290},
year = {2000}
}
@article{Breiman2001a,
author = {Breiman, Leo},
journal = {Machine learning},
number = {1},
pages = {5--32},
publisher = {Springer},
title = {{Random forests}},
volume = {45},
year = {2001}
}
@article{Tibshirani2002a,
abstract = {We have devised an approach to cancer class prediction from gene expression profiling, based on an enhancement of the simple nearest prototype (centroid) classifier. We shrink the prototypes and hence obtain a classifier that is often more accurate than competing methods. Our method of "nearest shrunken centroids" identifies subsets of genes that best characterize each class. The technique is general and can be used in many other classification problems. To demonstrate its effectiveness, we show that the method was highly efficient in finding genes for classifying small round blue cell tumors and leukemias.},
author = {Tibshirani, Robert and Hastie, Trevor and Narasimhan, Balasubramanian and Chu, Gilbert},
doi = {10.1073/pnas.082099299},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Child,DNA,Discriminant Analysis,Gene Expression,Gene Expression Profiling,Humans,Neoplasm,Neoplasm: analysis,Neoplasms,Neoplasms: classification,Neoplasms: diagnosis,Neoplasms: genetics,Precursor Cell Lymphoblastic Leukemia-Lymphoma,Precursor Cell Lymphoblastic Leukemia-Lymphoma: cl,Precursor Cell Lymphoblastic Leukemia-Lymphoma: di,Precursor Cell Lymphoblastic Leukemia-Lymphoma: ge,Probability},
month = may,
number = {10},
pages = {6567--6572},
pmid = {12011421},
title = {{Diagnosis of multiple cancer types by shrunken centroids of gene expression.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=124443\&tool=pmcentrez\&rendertype=abstract},
volume = {99},
year = {2002}
}
@book{Jolliffe2002a,
abstract = {Timmerman reviews Principal Component Analysis (2nd Ed.). I. T. Jolliffe},
author = {Jolliffe, I T},
booktitle = {Journal of the American Statistical Association},
doi = {10.1007/b98835},
isbn = {0-387-95442-2},
issn = {01621459},
keywords = {principal component analysis,statistical theory methods},
pages = {487},
title = {{Principal Component Analysis}},
url = {http://link.springer.com/10.1007/b98835},
volume = {98},
year = {2002}
}
@inproceedings{deSilva2003,
author = {de Silva, V and Tenenbaum, Joshua B},
booktitle = {Neural Information Processing Systems},
pages = {721--728},
title = {{Global Versus Local Methods in Nonlinear Dimensionality Reduction}},
year = {2003}
}
@article{Globerson2003a,
author = {Globerson, Amir and Tishby, Naftali},
doi = {10.1162/153244303322753689},
editor = {Guyon, Isabelle and Elisseeff, Andre},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {association analysis,feature extraction,information geometry,maximum entropy,mutual information},
month = oct,
number = {7-8},
pages = {1307--1331},
title = {{Sufficient Dimensionality Reduction}},
url = {http://www.crossref.org/jmlr\_DOI.html},
volume = {3},
year = {2003}
}
@article{Belkin2003a,
author = {Belkin, Mikhail and Niyogi, Partha},
journal = {Neural Computation},
pages = {1373--1396},
title = {{Laplacian Eigenmaps for Dimensionality Reduction and Data}},
volume = {15},
year = {2003}
}
@article{Fukumizu2004a,
author = {Fukumizu, Kenji and Bach, Francis R and Jordan, Michael I.},
journal = {Journal of Machine Learning Research},
keywords = {conditional independence,dimensionality reduction,feature selection,kernel methods,regression,variable selection},
pages = {73--99},
title = {{Dimensionality Reduction for Supervised Learning with Reproducing Kernel Hilbert Spaces}},
volume = {5},
year = {2004}
}
@article{Bickel2004a,
author = {Bickel, Peter J. and Levina, Elizaveta},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {Fisher's linear discriminant,Gaussian coloured noise,minimax regret,naive Bayes},
month = dec,
number = {6},
pages = {989--1010},
publisher = {Bernoulli Society for Mathematical Statistics and Probability},
title = {{Some theory for Fisher's linear discriminant function, `naive Bayes', and some alternatives when there are many more variables than observations}},
url = {http://projecteuclid.org/euclid.bj/1106314847},
volume = {10},
year = {2004}
}
@article{Hastie2004,
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
journal = {BeiJing: Publishing House of Electronics Industry},
title = {{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}},
year = {2004}
}
@article{Zhang2004f,
abstract = {We present a new algorithm for manifold learning and nonlinear dimensionality reduction. Based on a set of unorganized data points sampled with noise from a parameterized manifold, the local geometry of the manifold is learned by constructing an approximation for the tangent space at each data point, and those tangent spaces are then aligned to give the global coordinates of the data points with respect to the underlying manifold. We also present an error analysis of our algorithm showing that reconstruction errors can be quite small in some cases. We illustrate our algorithm using curves and surfaces both in two-dimensional/three-dimensional (2D/3D) Euclidean spaces and in higher-dimensional Euclidean spaces. We also address several theoretical and algorithmic issues for further research and improvements.},
author = {Zhang, Zhenyue and Zha, Hongyuan},
doi = {10.1137/S1064827502419154},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {10,1137,15A18,15A23,15a18,15a23,65F15,65F50,65f15,65f50,alignment,ams subject classifications,doi,nonlinear dimensionality reduction,principal manifold,s1064827502419154,singular value decomposition,subspace,subspace alignment,tangent space},
language = {en},
month = jan,
number = {1},
pages = {313--338},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment}},
url = {http://epubs.siam.org/doi/abs/10.1137/S1064827502419154},
volume = {26},
year = {2004}
}
@article{Cook2005a,
abstract = {A family of dimension-reduction methods, the inverse regression (IR) family, is developed by minimizing a quadratic objective function. An optimal member of this family, the inverse regression estimator (IRE), is proposed, along with inference methods and a computational algorithm. The IRE has at least three desirable properties: (1) Its estimated basis of the central dimension reduction subspace is asymptotically efficient, (2) its test statistic for dimension has an asymptotic chi-squared distribution, and (3) it provides a chi-squared test of the conditional independence hypothesis that the response is independent of a selected subset of predictors given the remaining predictors. Current methods like sliced inverse regression belong to a suboptimal class of the IR family. Comparisons of these methods are reported through simulation studies. The approach developed here also allows a relatively straightforward derivation of the asymptotic null distribution of the test statistic for dimension used in slic...},
author = {Cook, R. Dennis and Ni, Liqiang},
doi = {10.1198/016214504000001501},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Inverse regression estimator,Sliced average variance estimation,Sliced inverse regression,Sufficient dimension reduction},
language = {en},
month = jun,
number = {470},
pages = {410--428},
publisher = {Taylor \& Francis},
title = {{Sufficient Dimension Reduction via Inverse Regression}},
url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214504000001501\#.U6tH3Y1dUts},
volume = {100},
year = {2005}
}
@article{Coifman2006a,
author = {Coifman, Ronald R and Lafon, S},
doi = {10.1016/j.acha.2006.04.006},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
keywords = {diffusion metric,diffusion processes,dimensionality reduction,eigenmaps,graph laplacian,manifold learning},
month = jul,
number = {1},
pages = {5--30},
title = {{Diffusion maps}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1063520306000546},
volume = {21},
year = {2006}
}
@article{Belkin2006a,
abstract = {We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework. Keywords:},
author = {Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
doi = {10.1016/j.neuropsychologia.2009.02.028},
file = {:Users/jovo/Research/Articles/Belkin, Niyogi, Sindhwani - 2006.pdf:pdf;:Users/jovo/Research/Articles/Belkin, Niyogi, Sindhwani - 2006(2).pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {graph transduction,kernel methods,mani- fold learning,regularization,semi-supervised learning,spectral graph theory,support vector machines,unlabeled data},
month = dec,
pages = {2399--2434},
pmid = {19428409},
publisher = {JMLR.org},
title = {{Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples}},
url = {http://dl.acm.org/citation.cfm?id=1248547.1248632 http://dl.acm.org/citation.cfm?id=1248632},
volume = {7},
year = {2006}
}
@misc{Zou2006a,
abstract = {The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ?1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection. The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ?1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection.},
author = {Zou, Hui},
booktitle = {Journal of the American Statistical Association},
doi = {10.1198/016214506000000735},
isbn = {0162-1459},
issn = {0162-1459},
pages = {1418--1429},
title = {{The Adaptive Lasso and Its Oracle Properties}},
volume = {101},
year = {2006}
}
@article{Baik2006a,
abstract = {We consider a spiked population model, proposed by Johnstone, in which all the population eigenvalues are one except for a few fixed eigenvalues. The question is to determine how the sample eigenvalues depend on the non-unit population ones when both sample size and population size become large. This paper completely determines the almost sure limits of the sample eigenvalues in a spiked model for a general class of samples. © 2005 Elsevier Inc. All rights reserved.},
author = {Baik, Jinho and Silverstein, Jack W.},
journal = {Journal of Multivariate Analysis},
keywords = {Almost sure limits,Eigenvalues,Non-null case,Sample covariance matrices,Spiked population models},
number = {6},
pages = {1382--1408},
title = {{Eigenvalues of large sample covariance matrices of spiked population models}},
volume = {97},
year = {2006}
}
@article{Candes2006a,
author = {Cand\`{e}s, Emmanuel J.},
journal = {Proceedings of the International Congress of Mathematicians, Madrid, Spain},
pages = {1433--1452},
title = {{Compressive sampling}},
volume = {3},
year = {2006}
}
@book{Bishop2006a,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
chapter = {Graphical},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
isbn = {0387310738},
issn = {10179909},
pages = {738},
publisher = {Springer},
series = {Information science and statistics},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
year = {2006}
}
@article{Paul2007a,
abstract = {This paper deals with a multivariate Gaussian observation model where the eigenvalues of the covariance matrix are all one, except for a finite number which are larger. Of interest is the asymptotic behavior of the eigenvalues of the sample covariance matrix when the sample size and the dimension of the obser- vations both grow to infinity so that their ratio converges to a positive constant. When a population eigenvalue is above a certain threshold and of multiplicity one, the corresponding sample eigenvalue has a Gaussian limiting distribution. There is a “phase transition” of the sample eigenvectors in the same setting. Another contribution here is a study of the second order asymptotics of sample eigenvectors when corresponding eigenvalues are simple and sufficiently large.},
author = {Paul, Debashis},
issn = {1017-0405},
journal = {Statistica Sinica},
keywords = {and phrases,eigenvalue distribution,principal component analysis,random matrix theory},
number = {4},
pages = {1617},
title = {{Asymptotics of sample eigenstructure for a large dimensional spiked covariance model}},
url = {http://www3.stat.sinica.edu.tw/statistica/oldpdf/A17n418.pdf},
volume = {17},
year = {2007}
}
@article{Bouveyron2007,
author = {Bouveyron, Charles and Girard, S. and Schmid, C.},
doi = {10.1016/j.csda.2007.02.009},
issn = {01679473},
journal = {Computational Statistics \& Data Analysis},
keywords = {gaussian mixture models,high-dimensional data,model-based clustering,parsimonious models,subspace clustering},
month = sep,
number = {1},
pages = {502--519},
title = {{High-dimensional data clustering}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947307000692},
volume = {52},
year = {2007}
}
@article{Fan2008a,
author = {Fan, Jianqing and Fan, Yingying},
issn = {2168-8966},
journal = {The Annals of Statistics},
keywords = {Classification,feature extraction,high dimensionality,independence rule,misclassification rates},
language = {EN},
month = dec,
number = {6},
pages = {2605--2637},
publisher = {Institute of Mathematical Statistics},
title = {{High-dimensional classification using features annealed independence rules}},
url = {http://projecteuclid.org/euclid.aos/1231165181},
volume = {36},
year = {2008}
}
@article{Donoho2008a,
abstract = {In important application fields today-genomics and proteomics are examples-selecting a small subset of useful features is crucial for success of Linear Classification Analysis. We study feature selection by thresholding of feature Z-scores and introduce a principle of threshold selection, based on the notion of higher criticism (HC). For i = 1, 2, ..., p, let pi(i) denote the two-sided P-value associated with the ith feature Z-score and pi((i)) denote the ith order statistic of the collection of P-values. The HC threshold is the absolute Z-score corresponding to the P-value maximizing the HC objective (i/p - pi((i)))/sqrt\{i/p(1-i/p)\}. We consider a rare/weak (RW) feature model, where the fraction of useful features is small and the useful features are each too weak to be of much use on their own. HC thresholding (HCT) has interesting behavior in this setting, with an intimate link between maximizing the HC objective and minimizing the error rate of the designed classifier, and very different behavior from popular threshold selection procedures such as false discovery rate thresholding (FDRT). In the most challenging RW settings, HCT uses an unconventionally low threshold; this keeps the missed-feature detection rate under better control than FDRT and yields a classifier with improved misclassification performance. Replacing cross-validated threshold selection in the popular Shrunken Centroid classifier with the computationally less expensive and simpler HCT reduces the variance of the selected threshold and the error rate of the constructed classifier. Results on standard real datasets and in asymptotic theory confirm the advantages of HCT.},
author = {Donoho, David L. and Jin, Jiashun},
doi = {10.1073/pnas.0807471105},
file = {::},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Bias (Epidemiology),Data Collection,Data Collection: statistics \& numerical data,Genomics,Genomics: statistics \& numerical data,Linear Models,Proteomics,Proteomics: statistics \& numerical data},
month = sep,
number = {39},
pages = {14790--5},
pmid = {18815365},
title = {{Higher criticism thresholding: Optimal feature selection when useful features are rare and weak.}},
url = {http://www.pnas.org/content/105/39/14790 http://www.pnas.org/content/105/39/14790.short},
volume = {105},
year = {2008}
}
@article{Candes2009b,
abstract = {This paper is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the L1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
annote = {From Duplicate 2 ( },
archivePrefix = {arXiv},
arxivId = {0912.3599},
author = {Cand\`{e}s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
doi = {10.1145/1970392.1970395},
eprint = {0912.3599},
issn = {00045411},
journal = {Journal of the ACM},
keywords = {Principal components,duality,low-rank matrices,minimization,nuclear-norm minimization,principal components,robustness vis-a-vis outliers,sparsity,video surveillance,ℓ 1 -norm minimization},
month = dec,
number = {3},
pages = {1--37},
publisher = {ACM},
title = {{Robust Principal Component Analysis?}},
url = {http://dl.acm.org/citation.cfm?id=1970392.1970395 http://arxiv.org/abs/0912.3599},
volume = {58},
year = {2009}
}
@article{Witten2009a,
abstract = {In recent years, many methods have been developed for regression in high-dimensional settings. We propose covariance-regularized regression, a family of methods that use a shrunken estimate of the inverse covariance matrix of the features in order to achieve superior prediction. An estimate of the inverse covariance matrix is obtained by maximizing its log likelihood, under a multivariate normal model, subject to a constraint on its elements; this estimate is then used to estimate coefficients for the regression of the response onto the features. We show that ridge regression, the lasso, and the elastic net are special cases of covariance-regularized regression, and we demonstrate that certain previously unexplored forms of covariance-regularized regression can outperform existing methods in a range of situations. The covariance-regularized regression framework is extended to generalized linear models and linear discriminant analysis, and is used to analyze gene expression data sets with multiple class and survival outcomes.},
author = {Witten, Daniela M. and Tibshirani, Robert},
doi = {10.1111/j.1467-9868.2009.00699.x},
file = {::},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
month = feb,
number = {3},
pages = {615--636},
pmid = {20084176},
title = {{Covariance-regularized regression and classification for high-dimensional problems.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2806603\&tool=pmcentrez\&rendertype=abstract},
volume = {71},
year = {2009}
}
@inproceedings{Mairal2009,
author = {Mairal, Julien and Ponce, Jean and Sapiro, Guillermo and Zisserman, Andrew and Bach, Francis R.},
booktitle = {Advances in Neural Information Processing Systems},
file = {::},
pages = {1033--1040},
title = {{Supervised Dictionary Learning}},
url = {http://papers.nips.cc/paper/3448-supervised},
year = {2009}
}
@article{Halko2011a,
abstract = {Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed—either explicitly or implicitly—to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or spee...},
author = {Halko, N. and Martinsson, P. G. and Tropp, Joel A.},
doi = {10.1137/090771806},
issn = {0036-1445},
journal = {SIAM Review},
keywords = {60B20,65F30,68W20,Johnson–Lindenstrauss lemma,dimension reduction,eigenvalue decomposition,interpolative decomposition,matrix approximation,parallel algorithm,pass-efficient algorithm,principal component analysis,random matrix,randomized algorithm,rank-revealing QR factorization,singular value decomposition,streaming algorithm},
language = {en},
month = jan,
number = {2},
pages = {217--288},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions}},
url = {http://epubs.siam.org/doi/abs/10.1137/090771806},
volume = {53},
year = {2011}
}
@inproceedings{Lopes2011a,
author = {Lopes, Miles and Jacob, Laurent and Wainwright, Martin J.},
booktitle = {Neural Information Processing Systems},
file = {::},
pages = {1206--1214},
title = {{A More Powerful Two-Sample Test in High Dimensions using Random Projection}},
url = {http://papers.nips.cc/paper/4260-a-more-powerful-two-sample-test-in-high-dimensions-using-random-projection},
year = {2011}
}
@article{Chang2011a,
author = {Chang, Chih-Chung and Lin, Chih-Jen},
doi = {10.1145/1961189.1961199},
file = {::},
issn = {21576904},
journal = {ACM Transactions on Intelligent Systems and Technology},
keywords = {Classification LIBSVM optimization regression supp},
month = apr,
number = {3},
pages = {1--27},
publisher = {ACM},
title = {{LIBSVM}},
url = {http://dl.acm.org/citation.cfm?id=1961189.1961199},
volume = {2},
year = {2011}
}
@article{Clemmensen2011a,
author = {Clemmensen, Line and Hastie, Trevor and Witten, Daniela M. and Ersb\o ll, Bjarne},
doi = {10.1198/TECH.2011.08118},
issn = {0040-1706},
journal = {Technometrics},
month = nov,
number = {4},
pages = {406--413},
title = {{Sparse Discriminant Analysis}},
url = {http://pubs.amstat.org/doi/abs/10.1198/TECH.2011.08118},
volume = {53},
year = {2011}
}
@article{Eklund2012,
abstract = {The validity of parametric functional magnetic resonance imaging (fMRI) analysis has only been reported for simulated data. Recent advances in computer science and data sharing make it possible to analyze large amounts of real fMRI data. In this study, 1484 rest datasets have been analyzed in SPM8, to estimate true familywise error rates. For a familywise significance threshold of 5\%, significant activity was found in 1\%-70\% of the 1484 rest datasets, depending on repetition time, paradigm and parameter settings. This means that parametric significance thresholds in SPM both can be conservative or very liberal. The main reason for the high familywise error rates seems to be that the global AR(1) auto correlation correction in SPM fails to model the spectra of the residuals, especially for short repetition times. The findings that are reported in this study cannot be generalized to parametric fMRI analysis in general, and other software packages may give different results. By using the computational power of the graphics processing unit (GPU), the 1484 rest datasets were also analyzed with a random permutation test. Significant activity was then found in 1\%-19\% of the datasets. These findings speak to the need for a better model of temporal correlations in fMRI timeseries.},
author = {Eklund, Anders and Andersson, Mats and Josephson, Camilla and Johannesson, Magnus and Knutsson, Hans},
doi = {10.1016/j.neuroimage.2012.03.093},
issn = {1095-9572},
journal = {NeuroImage},
keywords = {Familywise error rate,Functional magnetic resonance imaging (fMRI),Graphics processing unit (GPU),Non-parametric statistics,Random field theory,Random permutation test,functional magnetic resonance imaging},
month = apr,
number = {3},
pages = {565--578},
pmid = {22507229},
publisher = {Elsevier Inc.},
title = {{Does parametric fMRI analysis with SPM yield valid results?-An empirical study of 1484 rest datasets.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22507229},
volume = {61},
year = {2012}
}
@article{Allard2012,
author = {Allard, William K. and Chen, Guangliang and Maggioni, Mauro},
doi = {10.1016/j.acha.2011.08.001},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
keywords = {multi-scale analysis},
month = may,
number = {3},
pages = {435--462},
publisher = {Elsevier Inc.},
title = {{Multi-scale geometric methods for data sets II: Geometric Multi-Resolution Analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1063520311000868},
volume = {32},
year = {2012}
}
@article{Fan2012a,
author = {Fan, Jianqing and Feng, Yang and Tong, Xin},
doi = {10.1111/j.1467-9868.2012.01029.x},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
month = sep,
number = {4},
pages = {745--771},
title = {{A road to classification in high dimensional space: the regularized optimal affine discriminant}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2012.01029.x},
volume = {74},
year = {2012}
}
@article{Cook2013,
author = {Cook, R. Dennis and Forzani, Liliana and Rothman, Adam J.},
journal = {Electronic Journal of Statistics},
pages = {3059--3088},
title = {{Prediction in abundant high-dimensional linear regression}},
url = {https://projecteuclid.org/euclid.ejs/1387207935},
volume = {7},
year = {2013}
}

@ARTICLE{,
  title     = "On the Equivalence between Canonical Correlation Analysis and
               Orthonormalized Partial Least Squares",
  author    = "Sun, L and Ji, S and Yu, S and Ye, J",
  abstract  = "Abstract Canonical correlation analysis (CCA) and partial least
               squares (PLS) are well- known techniques for feature extraction
               from two sets of multidimensional variables. The fundamental
               difference between CCA and PLS is that CCA maximizes the
               correlation while",
  journal   = "IJCAI",
  publisher = "aaai.org",
  year      =  2009
}

@article{Mai2013a,
abstract = {Abstract In this paper we reveal the connection and equivalence of three sparse linear discriminant analysis methods : the ℓ1-Fisher's discriminant analysis proposed in Wu et al.(2008), the sparse optimal scoring proposed in Clemmensen et al.(2011) and the direct ... $\backslash$n},
author = {Mai, Qing and Zou, Hui},
doi = {10.1080/00401706.2012.746208},
issn = {0040-1706},
journal = {Technometrics},
pages = {243--246},
title = {{A Note On the Connection and Equivalence of Three Sparse Linear Discriminant Analysis Methods}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.2012.746208},
volume = {55},
year = {2013}
}
@article{DiMartino2013a,
abstract = {Autism spectrum disorders (ASDs) represent a formidable challenge for psychiatry and neuroscience because of their high prevalence, lifelong nature, complexity and substantial heterogeneity. Facing these obstacles requires large-scale multidisciplinary efforts. Although the field of genetics has pioneered data sharing for these reasons, neuroimaging had not kept pace. In response, we introduce the Autism Brain Imaging Data Exchange (ABIDE)—a grassroots consortium aggregating and openly sharing 1112 existing resting-state functional magnetic resonance imaging (R-fMRI) data sets with corresponding structural MRI and phenotypic information from 539 individuals with ASDs and 573 age-matched typical controls (TCs; 7–64 years) (http://fcon\_1000.projects.nitrc.org/indi/abide/). Here, we present this resource and demonstrate its suitability for advancing knowledge of ASD neurobiology based on analyses of 360 male subjects with ASDs and 403 male age-matched TCs. We focused on whole-brain intrinsic functional connectivity and also survey a range of voxel-wise measures of intrinsic functional brain architecture. Whole-brain analyses reconciled seemingly disparate themes of both hypo- and hyperconnectivity in the ASD literature; both were detected, although hypoconnectivity dominated, particularly for corticocortical and interhemispheric functional connectivity. Exploratory analyses using an array of regional metrics of intrinsic brain function converged on common loci of dysfunction in ASDs (mid- and posterior insula and posterior cingulate cortex), and highlighted less commonly explored regions such as the thalamus. The survey of the ABIDE R-fMRI data sets provides unprecedented demonstrations of both replication and novel discovery. By pooling multiple international data sets, ABIDE is expected to accelerate the pace of discovery setting the stage for the next generation of ASD studies.},
author = {{Di Martino}, Adriana and Yan, C-G and Li, Q and Denio, E and Castellanos, Xavier Francisco and Alaerts, K and Anderson, J S and Assaf, M and Bookheimer, S Y and Dapretto, M and Deen, B and Delmonte, S and Dinstein, I and Ertl-Wagner, B and Fair, D A and Gallagher, L and Kennedy, D P and Keown, C L and Keysers, C and Lainhart, J E and Lord, C and Luna, B and Menon, V and Minshew, N J and Monk, C S and Mueller, S and M\"{u}ller, R-A and Nebel, M B and Nigg, J T and O'Hearn, K and Pelphrey, K A and Peltier, S J and Rudie, J D and Sunaert, S and Thioux, M and Tyszka, J M and Uddin, L Q and Verhoeven, J S and Wenderoth, N and Wiggins, J L and Mostofsky, Stewart H and Milham, Michael Peter},
doi = {10.1038/mp.2013.78},
issn = {1359-4184},
journal = {Molecular Psychiatry},
keywords = {data sharing,default network,interhemispheric connectivity,intrinsic functional connectivity,resting-state fMRI,thalamus},
month = jun,
publisher = {Macmillan Publishers Limited},
shorttitle = {Mol Psychiatry},
title = {{The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism}},
url = {http://dx.doi.org/10.1038/mp.2013.78},
year = {2013}
}
@article{Zhang2014a,
author = {Zhang, Teng and Lerman, Gilad},
file = {::},
journal = {Journal of Machine Learning Research},
pages = {749--808},
title = {{A Novel M-Estimator for Robust PCA}},
url = {http://jmlr.org/papers/v15/zhang14a.html},
volume = {15},
year = {2014}
}
@misc{mnist,
author = {LeCun, Yann and Cortes, Corinna and Burges, Chris},
title = {{MNIST handwritten digit database}},
url = {http://yann.lecun.com/exdb/mnist/},
urldate = {2015-07-01}
}

@article{FlashMatrix,
	title={FlashMatrix: Parallel, Scalable Data Analysis with Generalized Matrix Operations using Commodity SSDs},
	author={Zheng, Da and Mhembere, Disa and Vogelstein, Joshua T and Priebe, Carey E and Burns, Randal},
	journal={arXiv preprint arXiv:1604.06414},
	year={2016}
}

@article{Breiman2001b,
	Author = {L. Breiman},
	Date-Added = {2014-07-22 14:05:28 +0000},
	Date-Modified = {2014-07-22 14:06:29 +0000},
	Journal = {Statistical Science},
	Pages = {199-231},
	number = {3},
	Title = {Statistical Modeling: The Two Cultures},
	Volume = {16},
	Year = {2001}}



@ARTICLE{Barshan2011,
  title    = "Supervised principal component analysis: Visualization,
              classification and regression on subspaces and submanifolds",
  author   = "Barshan, Elnaz and Ghodsi, Ali and Azimifar, Zohreh and Zolghadri
              Jahromi, Mansoor",
  abstract = "We propose ``supervised principal component analysis (supervised
              PCA)'', a generalization of PCA that is uniquely effective for
              regression and classification problems with high-dimensional
              input data. It works by estimating a sequence of principal
              components that have maximal dependence on the response variable.
              The proposed supervised PCA is solvable in closed-form, and has a
              dual formulation that significantly reduces the computational
              complexity of problems in which the number of predictors greatly
              exceeds the number of observations (such as DNA microarray
              experiments). Furthermore, we show how the algorithm can be
              kernelized, which makes it applicable to non-linear
              dimensionality reduction tasks. Experimental results on various
              visualization, classification and regression problems show
              significant improvement over other supervised approaches both in
              accuracy and computational efficiency.",
  journal  = "Pattern Recognit.",
  volume   =  44,
  number   =  7,
  pages    = "1357--1371",
  month    =  "jul",
  year     =  2011,
  keywords = "Dimensionality reduction; Principal component analysis (PCA);
              Kernel methods; Supervised learning; Visualization;
              Classification; Regression"
}


@ARTICLE{Bair2006,
  title     = "Prediction by Supervised Principal Components",
  author    = "Bair, Eric and Hastie, Trevor and Paul, Debashis and Tibshirani,
               Robert",
  abstract  = "In regression problems where the number of predictors greatly
               exceeds the number of observations, conventional regression
               techniques may produce unsatisfactory results. We describe a
               technique called supervised principal components that can be
               applied to this type of problem. Supervised principal components
               is similar to conventional principal components analysis except
               that it uses a subset of the predictors selected based on their
               association with the outcome. Supervised principal components
               can be applied to regression and generalized regression
               problems, such as survival analysis. It compares favorably to
               other techniques for this type of problem, and can also account
               for the effects of other covariates and help identify which
               predictor variables are most important. We also provide
               asymptotic consistency results to help support our empirical
               findings. These methods could become important tools for DNA
               microarray data, where they may be used to more accurately
               diagnose and treat cancer.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  101,
  number    =  473,
  pages     = "119--137",
  month     =  "mar",
  year      =  2006
}


@inproceedings{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/jovo/Research/Articles/Krizhevsky, Sutskever, Hinton - 2012.pdf:pdf},
pages = {1097--1105},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-w},
year = {2012}
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Bengio2004,
  title     = "{Out-of-Sample} Extensions for {LLE}, Isomap, {MDS}, Eigenmaps,
               and Spectral Clustering",
  booktitle = "Advances in Neural Information Processing Systems 16",
  author    = "Bengio, Yoshua and Paiement, Jean-Fran{\c c}cois and Vincent,
               Pascal and Delalleau, Olivier and Roux, Nicolas L and Ouimet,
               Marie",
  editor    = "Thrun, S and Saul, L K and Sch{\"o}lkopf, P B",
  abstract  = "... sition provide either an embedding or a clustering only for
               given train- ing points, with no straightforward extension for
               out -of- sample examples short ... 3. Compute the m largest
               positive eigenvalues $\lambda$k and eigenvectors vk of ˜M. 4.
               The embedding of each example xi is the vector ...",
  publisher = "MIT Press",
  pages     = "177--184",
  year      =  2004
}


@ARTICLE{Su2015,
  title    = "False Discoveries Occur Early on the Lasso Path",
  author   = "Su, Weijie and Bogdan, Malgorzata and Candes, Emmanuel",
  abstract = "In regression settings where explanatory variables have very low
              correlations and where there are relatively few effects each of
              large magnitude, it is commonly believed that the Lasso shall be
              able to find the important variables with few errors---if any. In
              contrast, this paper shows that this is not the case even when
              the design variables are stochastically independent. In a regime
              of linear sparsity, we demonstrate that true features and null
              features are always interspersed on the Lasso path, and that this
              phenomenon occurs no matter how strong the effect sizes are. We
              derive a sharp asymptotic trade-off between false and true
              positive rates or, equivalently, between measures of type I and
              type II errors along the Lasso path. This trade-off states that
              if we ever want to achieve a type II error (false negative rate)
              under a given threshold, then anywhere on the Lasso path the type
              I error (false positive rate) will need to exceed a given
              threshold so that we can never have both errors at a low level at
              the same time. Our analysis uses tools from approximate message
              passing (AMP) theory as well as novel elements to deal with a
              possibly adaptive selection of the Lasso regularizing parameter.",
  month    =  "nov",
  journal  = "arXiv",
  year     =  {2015}
}


@BOOK{Hastie2015,
  title     = "Statistical Learning with Sparsity: The Lasso and
               Generalizations (Chapman \& {Hall/CRC} Monographs on Statistics
               \& Applied Probability)",
  author    = "Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin",
  publisher = "Chapman and Hall/CRC",
  edition   = "1 edition",
  month     =  "may",
  year      =  {2015},
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Jolliffe1986,
  title     = "Principal Component Analysis and Factor Analysis",
  booktitle = "Principal Component Analysis",
  author    = "Jolliffe, I T",
  abstract  = "Principal component analysis has often been dealt with in
               textbooks as a special case of factor analysis, and this
               tendency has been continued by many computer packages which
               treat PCA as one option in a program for factor analysis---see
               Appendix A2. This view is misguided since PCA and factor
               analysis, as usually defined, are really quite distinct
               techniques. The confusion may have arisen, in part, because of
               Hotelling’s (1933) original paper, in which principal components
               were introduced in the context of providing a small number of
               ‘more fundamental’ variables which determine the values of the p
               original variables. This is very much in the spirit of the
               factor model introduced in Section 7.1, although Girschick
               (1936) indicates that there were soon criticisms of Hotelling’s
               method of PCs, as being inappropriate for factor analysis.
               Further confusion results from the fact that practitioners of
               ‘factor analysis’ do not always have the same definition of the
               technique (see Jackson, 1981). The definition adopted in this
               chapter is, however, fairly standard.",
  publisher = "Springer, New York, NY",
  pages     = "115--128",
  series    = "Springer Series in Statistics",
  year      =  1986,
  language  = "en"
}


@article{Vogelstein2014a,
	Author = {J. T. Vogelstein and Y. Park and T. Ohyama and R. Kerr and J. Truman and C. E. Priebe and M. Zlatic},
	Date-Added = {2014-07-31 14:56:17 +0000},
	Date-Modified = {2014-07-31 14:57:13 +0000},
	Journal = {Science},
	Number = {6182},
	Pages = {386-392},
	Title = {Discovery of brainwide neural-behavioral maps via multiscale unsupervised structure learning},
	Volume = {344},
	Year = {2014}}

@article{SEM_SpMM,
	author    = {Da Zheng and Disa Mhembere and Vince Lyzinski and Joshua Vogelstein and Carey E. Priebe and Randal Burns},
	title     = {Semi-External Memory Sparse Matrix Multiplication on Billion-node Graphs in a Multicore Architecture},
	journal   = {CoRR},
	volume    = {abs/1602.02864},
	year      = {2016},
}

@article{FlashEigen,
	author    = {Da Zheng and Randal Burns and Joshua Vogelstein and Carey E. Priebe and Alexander S. Szalay},
	title     = {An SSD-based eigensolver for spectral analysis on billion-node graphs},
	journal   = {CoRR},
	volume    = {abs/1602.01421},
	year      = {2016},
}

@inproceedings {FlashGraph,
	author = {Da Zheng and Disa Mhembere and Randal Burns and Joshua Vogelstein and Carey E. Priebe and Alexander S. Szalay},
	title = {{FlashGraph}: Processing Billion-Node Graphs on an Array of Commodity {SSDs}},
	booktitle = {13th USENIX Conference on File and Storage Technologies (FAST 15)},
	year = {2015},
	address = {Santa Clara, CA},
}

@inproceedings{SAFS,
	author = {Zheng, Da and Burns, Randal and Szalay, Alexander S.},
	title = {Toward Millions of File System IOPS on Low-cost, Commodity Hardware},
	booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
	year = {2013},
	location = {Denver, Colorado},
}
